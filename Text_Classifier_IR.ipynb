{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hrishikesh-Harsh/Text_Classification_IR/blob/main/Text_Classifier_IR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use TF-IDF as prescribed\n",
        "### Treat Abstract, Key and Title differently\n",
        "### Order: $Key > Abstract â‰ˆ Title$\n",
        "### Hence, $W_k > W_a \\approx W_t$\n",
        "### For TF, we can use $TF = 1 + log(n_t*W_t + n_k*W_k + n_a*W_a)$\n",
        "### One More Hyper-parameter to vary is Window size for (Word,Word) pairs\n",
        "### Use $3$ different Window sizes for $Key, Abstract, Title$: $Win_k, Win_a, Win_t$"
      ],
      "metadata": {
        "id": "bvuRuAI6OwL1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXkm6ja9Rone"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65J4-wpWymj-"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/yao8839836/text_gcn.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd text_gcn"
      ],
      "metadata": {
        "id": "hd246BWFzLUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sSg_DXcQMJt",
        "outputId": "43cc0771-ac2a-426c-d0fc-be7465408097"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 remove_words.py 20ng"
      ],
      "metadata": {
        "id": "pVVvzv__QMG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python build_graph.py 20ng"
      ],
      "metadata": {
        "id": "9zGBuFSYQMMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kMJj6iPgQMPv",
        "outputId": "1af75bc2-e4c6-4020-cb17-c0a15cd76df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import math\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize "
      ],
      "metadata": {
        "id": "3hihn_ln9prw",
        "outputId": "0ca80e13-ab03-4baa-e816-31df46db9af6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "3T-H9t6EF3AQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading original .csv file\n",
        "\n",
        "file = open('/content/drive/MyDrive/IR_Project/dataset/PubMed.csv',encoding='Latin1')\n",
        "type(file)\n",
        "csvreader = csv.reader(file)"
      ],
      "metadata": {
        "id": "WFX7tvxa-G_h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set of stopwords to be removed\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "wAdO0mSn-HFS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.7\n",
        "test_ratio = 0.3\n",
        "N = 207\n",
        "\n",
        "train_N = int(train_ratio*N)\n",
        "test_N = int(test_ratio*N)"
      ],
      "metadata": {
        "id": "mV2vTRnuvhKw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just the first line of the .csv file (Column Names)\n",
        "header = []\n",
        "header = next(csvreader)"
      ],
      "metadata": {
        "id": "xKFHS9krAgxu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_words would be a set of all distinct words found in the dataset/.csv file minus the stopwords\n",
        "vocab_words = set()\n",
        "vocab_words_list = []"
      ],
      "metadata": {
        "id": "tAzHoVJ6JEZq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "index = 0\n",
        "title = []\n",
        "keyword = []\n",
        "abstract = []\n",
        "for r in csvreader: \n",
        "  r_sub = []\n",
        "  r[2] = r[2].replace('.',' ')\n",
        "  r[2] = r[2].replace(',',' ')\n",
        "  r[2] = r[2].replace(';',' ')\n",
        "  r[2] = r[2].replace('|',' ')\n",
        "  r[2] = r[2].replace('<',' ')\n",
        "  r[2] = r[2].replace('>',' ')\n",
        "  r[2] = r[2].replace(':',' ')\n",
        "  r[2] = r[2].replace('=',' ')\n",
        "  r[2] = r[2].replace('(',' ')\n",
        "  r[2] = r[2].replace(')',' ')\n",
        "  r[2] = r[2].replace('[',' ')\n",
        "  r[2] = r[2].replace(']',' ')\n",
        "  r[2] = r[2].replace('?',' ')\n",
        "\n",
        "\n",
        "  r[3] = r[3].replace('.',' ')\n",
        "  r[3] = r[3].replace(',',' ')\n",
        "  r[3] = r[3].replace(';',' ')\n",
        "  r[3] = r[3].replace('|',' ')\n",
        "  r[3] = r[3].replace('<',' ')\n",
        "  r[3] = r[3].replace('>',' ')\n",
        "  r[3] = r[3].replace(':',' ')\n",
        "  r[3] = r[3].replace('=',' ')\n",
        "  r[3] = r[3].replace('(',' ')\n",
        "  r[3] = r[3].replace(')',' ')\n",
        "  r[3] = r[3].replace('[',' ')\n",
        "  r[3] = r[3].replace(']',' ')\n",
        "  r[3] = r[3].replace('?',' ')\n",
        "\n",
        "  r[4] = r[4].replace('.',' ')\n",
        "  r[4] = r[4].replace(',',' ')\n",
        "  r[4] = r[4].replace(';',' ')\n",
        "  r[4] = r[4].replace('|',' ')\n",
        "  r[4] = r[4].replace('<',' ')\n",
        "  r[4] = r[4].replace('>',' ')\n",
        "  r[4] = r[4].replace(':',' ')\n",
        "  r[4] = r[4].replace('=',' ')\n",
        "  r[4] = r[4].replace('(',' ')\n",
        "  r[4] = r[4].replace(')',' ')\n",
        "  r[4] = r[4].replace('[',' ')\n",
        "  r[4] = r[4].replace(']',' ')\n",
        "  r[4] = r[4].replace('?',' ')\n",
        "\n",
        "  words_title = word_tokenize(r[2])     # Tokenize the Title of that doc (row)\n",
        "  words_keyword = word_tokenize(r[3])   # Tokenize the Keywords of that doc (row)\n",
        "  words_abstract = word_tokenize(r[4])  # Tokenize the Abstract of that doc (row)\n",
        "\n",
        "  title.append([])\n",
        "  keyword.append([])\n",
        "  abstract.append([])\n",
        "\n",
        "  w_t = \"\"\n",
        "  w_k = \"\"\n",
        "  w_a = \"\"\n",
        "\n",
        "  ''' \n",
        "      - Adding all non-stop-words to the vocabulary\n",
        "      - Also maintaining doc-wise collection of Keywords, Title and Abstract words \n",
        "  '''\n",
        "\n",
        "  for w in words_title:\n",
        "    if w not in stop_words and w!=\"'s\":\n",
        "        w_t = w_t + w + \" \"\n",
        "        vocab_words.add(w)\n",
        "        # vocab_words_list.append(w)\n",
        "        title[index].append(w)\n",
        "  \n",
        "  for w in words_keyword:\n",
        "    if w not in stop_words and w!=\"'s\":\n",
        "        w_k = w_k + w + \" \"\n",
        "        vocab_words.add(w)\n",
        "        # vocab_words_list.append(w)\n",
        "        keyword[index].append(w)\n",
        "\n",
        "  for w in words_abstract:\n",
        "    if w not in stop_words and w!=\"'s\":\n",
        "        w_a = w_a + w + \" \"\n",
        "        vocab_words.add(w)\n",
        "        # vocab_words_list.append(w)\n",
        "        abstract[index].append(w)\n",
        "  \n",
        "  index=index+1\n",
        "\n",
        "  '''\n",
        "    - Creating 'rows' to write back to Clean File\n",
        "  '''\n",
        "  for i in range(0,len(r)):\n",
        "    if(i==2):\n",
        "      r_sub.append(w_t)\n",
        "    elif(i==3):\n",
        "      r_sub.append(w_k)\n",
        "    elif(i==4):\n",
        "      r_sub.append(w_a) \n",
        "    else:\n",
        "      r_sub.append(r[i])\n",
        "    \n",
        "  rows.append(r_sub)\n"
      ],
      "metadata": {
        "id": "sto5tlbyAwu_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_docs = []\n",
        "test_docs = []\n",
        "\n",
        "train_docs = rows[:train_N]\n",
        "test_docs = rows[train_N:]"
      ],
      "metadata": {
        "id": "DwPWp9DppA57"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for wd in vocab_words:\n",
        "  vocab_words_list.append(wd)"
      ],
      "metadata": {
        "id": "V6yddrnwBX3b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Globals\n",
        "docs_size = len(rows)\n",
        "vocab_size = len(vocab_words)\n",
        "wt_k = 3    # Weight to be given to Keywords in tf score\n",
        "wt_t = 1    # Weight to be given to Title in tf score\n",
        "wt_a = 1    # Weight to be given to Abstract in tf score\n",
        "window_size = 20 # Window size for PMI calculation"
      ],
      "metadata": {
        "id": "uTf_qtYyJ0kj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  To write back to Clean File\n",
        "'''\n",
        "file = open('/content/drive/MyDrive/IR_Project/dataset/PubMed_Clean.csv', 'w', newline='')\n",
        "writer = csv.writer(file)\n",
        "writer.writerow(header)\n",
        "\n",
        "for r in rows:\n",
        "  writer.writerow(r)\n"
      ],
      "metadata": {
        "id": "W_ymGTwQDA4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Create a dict to store {word: (ID, idf)} mapping\n",
        "'''\n",
        "dict_vocab = {}\n",
        "\n",
        "for i,w in enumerate(vocab_words):\n",
        "  dict_vocab[w]=(i,0)"
      ],
      "metadata": {
        "id": "xeAiaPkDIOBi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,w in enumerate(vocab_words):\n",
        "  if(i>5):\n",
        "    break\n",
        "  print(w)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "3vNg82HsthO6",
        "outputId": "fd766368-994c-484c-add2-ff16ad77e306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OSA\n",
            "Semi-automated\n",
            "3-4\n",
            "Image\n",
            "mortalities\n",
            "Bradycardia\n",
            "59464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/MyDrive/IR_Project/dataset/idf.csv', 'w', newline='') \n",
        "writer = csv.writer(file) \n",
        "writer.writerow([\"word\",\"idf\"])\n",
        "for wd in dict_vocab: \n",
        "  count = 0\n",
        "  for i,doc in enumerate(rows):\n",
        "    flag = 0 \n",
        "    for w_t in title[i]:\n",
        "      if(wd==w_t):\n",
        "        count=count+1\n",
        "        flag = 1\n",
        "        break \n",
        "\n",
        "    if(flag==1):\n",
        "      continue \n",
        "\n",
        "    for w_k in keyword[i]:\n",
        "      if(wd==w_k):\n",
        "        count=count+1\n",
        "        flag = 1\n",
        "        break\n",
        "\n",
        "    if(flag==1):\n",
        "      continue \n",
        "\n",
        "    for w_a in abstract[i]:\n",
        "      if(wd==w_a):\n",
        "        count=count+1\n",
        "        flag = 1\n",
        "        break\n",
        "\n",
        "    if(flag==1):\n",
        "      continue\n",
        "\n",
        "  (id,idf) = dict_vocab[wd]\n",
        "  if(count==0):\n",
        "    idf = 0\n",
        "  else:\n",
        "    idf = math.log((docs_size/count),10) \n",
        "  dict_vocab[wd] = (id,idf)\n",
        "  templine=[]\n",
        "  templine.append(wd)\n",
        "  templine.append(idf)\n",
        "  writer.writerow(templine)\n",
        "  print(id,templine)\n",
        "  count = 0\n",
        "writer.writerow(\"fine\")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "KozY8dLB8Lfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/MyDrive/IR_Project/dataset/idf.csv',encoding='Latin1')\n",
        "type(file)\n",
        "csvreader = csv.reader(file)\n",
        "\n",
        "header = []\n",
        "header = next(csvreader)"
      ],
      "metadata": {
        "id": "CohPXaSOrZ78"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for val in dict_vocab: \n",
        "  r = next(csvreader)\n",
        "  if(r[0] not in dict_vocab):\n",
        "    continue\n",
        "  if(r[1]=='i'):\n",
        "    break\n",
        "  (id,idf) = dict_vocab[r[0]]\n",
        "  idf = float(r[1])\n",
        "  dict_vocab[r[0]] = (id,idf)"
      ],
      "metadata": {
        "id": "N61pPwI5r4hS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,ele in enumerate(dict_vocab):\n",
        "  if(i==5):\n",
        "    break;\n",
        "  \n",
        "  print(ele,\",\",dict_vocab[ele])"
      ],
      "metadata": {
        "id": "pzCpsOziIZ4Q",
        "outputId": "3bf9af27-4b24-4a7c-df55-2fa3014c760a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HUVEC , (0, 3.9461082304369057)\n",
            "outcomes , (1, 1.0156686356702058)\n",
            "disease-associated , (2, 3.6450782347729245)\n",
            "Cas12a , (3, 3.6450782347729245)\n",
            "1087 , (4, 3.9461082304369057)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Adj_Matrix = np.zeros((docs_size+vocab_size,docs_size+vocab_size))\n",
        "weights = []\n",
        "row_list = []\n",
        "col_list = []"
      ],
      "metadata": {
        "id": "r9UXxGeAONi7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,doc in enumerate(rows):\n",
        "  for w_k in keyword[i]:\n",
        "    Adj_Matrix[i,dict_vocab[w_k][0]+docs_size] += wt_k\n",
        "    Adj_Matrix[dict_vocab[w_k][0]+docs_size,i] += wt_k\n",
        "\n",
        "  for w_t in title[i]:\n",
        "    Adj_Matrix[i,dict_vocab[w_t][0]+docs_size] += wt_t\n",
        "    Adj_Matrix[dict_vocab[w_t][0]+docs_size,i] += wt_t\n",
        "\n",
        "  for w_a in abstract[i]:\n",
        "    Adj_Matrix[i,dict_vocab[w_a][0]+docs_size] += wt_a\n",
        "    Adj_Matrix[dict_vocab[w_a][0]+docs_size,i] += wt_a\n",
        "  # wt = 0\n",
        "  # for w_k in keyword[i]:\n",
        "  #   wt += wt_k\n",
        "  #   weights.append(wt*dict_vocab[w_k][1])\n",
        "  #   row_list.append(i)\n",
        "  #   col_list.append(dict_vocab[w_k][0]+docs_size)\n",
        "  # for w_t in title[i]:\n",
        "  #   wt += wt_t\n",
        "  #   weights.append(wt*dict_vocab[w_t][1])\n",
        "  #   row_list.append(i)\n",
        "  #   col_list.append(dict_vocab[w_t][0]+docs_size)\n",
        "  # for w_a in abstract[i]:\n",
        "  #   wt += wt_a\n",
        "  #   weights.append(wt*dict_vocab[w_a][1])\n",
        "  #   row_list.append(i)\n",
        "  #   col_list.append(dict_vocab[w_a][0]+docs_size)\n"
      ],
      "metadata": {
        "id": "MvP0Cz_2OSHq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_BHcU_RVE6L1",
        "outputId": "2eac7ac3-da3b-4d02-dc30-150d15e8fe89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[65712, 54753, 45545, 61224, 48352]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adj_header = []\n",
        "for i in range(0,Adj_Matrix.shape[0]):\n",
        "  if(i>=docs_size):\n",
        "    adj_header.append(\"Word-\"+str(i-docs_size))\n",
        "  else:\n",
        "    adj_header.append(\"Doc-\"+str(i))"
      ],
      "metadata": {
        "id": "45-U417wyiOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/MyDrive/IR_Project/dataset/Adj_M.csv', 'w', newline='') \n",
        "writer = csv.writer(file) \n",
        "writer.writerow(adj_header)\n",
        "\n",
        "for i in range(0,Adj_Matrix.shape[0]):\n",
        "  row_i = []\n",
        "  for j in range(0,Adj_Matrix.shape[0]):\n",
        "    if(Adj_Matrix[i,j]!=0):\n",
        "      if(i<j):\n",
        "        wd = vocab_words_list[j-docs_size]\n",
        "        idf = dict_vocab[wd][1]\n",
        "        Adj_Matrix[i,j] = (1+math.log(Adj_Matrix[i,j],10))*idf\n",
        "        print(i,\",\",j)\n",
        "        # print(wd,\"|\",idf,\"|\",Adj_Matrix[i,j])\n",
        "      else:\n",
        "        wd = vocab_words_list[i-docs_size]\n",
        "        idf = dict_vocab[wd][1]\n",
        "        Adj_Matrix[i,j] = (1+math.log(Adj_Matrix[i,j],10))*idf\n",
        "        print(i,\",\",j)\n",
        "        # print(wd,\"|\",idf,\"|\",Adj_Matrix[i,j])\n",
        "    \n",
        "    row_i.append(Adj_Matrix[i,j])\n",
        "  \n",
        "  writer.writerow(row_i)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "akrynkoDu-Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word co-occurence with context windows\n",
        "windows = []\n",
        "\n",
        "for row in rows:\n",
        "    content=row[2]+\" \"+row[3]+\" \"+row[4]\n",
        "    words = content.split()\n",
        "    length = len(words)\n",
        "    if length <= window_size: \n",
        "        windows.append(words)\n",
        "    else:\n",
        "        # print(length, length - window_size + 1)\n",
        "        for j in range(length - window_size + 1):\n",
        "            window = words[j: j + window_size]\n",
        "            windows.append(window)\n",
        "            # print(window)\n",
        "\n",
        "print(len(windows))\n",
        "\n",
        "#calculating p(i) , word_window_freq has the number of windows a particular word appears in across all windows.\n",
        "word_window_freq = {}\n",
        "k = 0\n",
        "for window in windows:\n",
        "    appeared = set()\n",
        "    for i in range(len(window)):\n",
        "        if window[i] in appeared:\n",
        "            continue\n",
        "        if window[i] in word_window_freq:\n",
        "            word_window_freq[window[i]] += 1\n",
        "        else:\n",
        "            word_window_freq[window[i]] = 1\n",
        "        appeared.add(window[i])\n",
        "    # print(\"k=\",k)  \n",
        "    # k+=1\n",
        "print(len(word_window_freq))\n",
        "\n",
        "word_pair_count = {}\n",
        "k1=0\n",
        "for window in windows:\n",
        "    for i in range(1, len(window)):\n",
        "        for j in range(0, i):\n",
        "            word_i = window[i] #ith word in window\n",
        "            word_i_id = dict_vocab[word_i][0]\n",
        "            word_j = window[j] #jth word in range 0-i in the same window\n",
        "            word_j_id = dict_vocab[word_j][0]\n",
        "            if word_i_id == word_j_id:\n",
        "                continue\n",
        "            word_pair_str = str(word_i_id) + ',' + str(word_j_id) #concat id and use it to count a pair or p(i,j)\n",
        "            if word_pair_str in word_pair_count: #word_pair_count stores number of pairs along with number of times they appear.\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1\n",
        "            # two orders\n",
        "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
        "            if word_pair_str in word_pair_count:\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1\n",
        "    # k1+=1\n",
        "    # print(\"k1=\",k1)    \n",
        "print(len(word_pair_count))\n",
        "\n",
        "# row = []\n",
        "# col = []\n",
        "# weight = []\n",
        "\n",
        "# pmi as weights\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of6lRt0325On",
        "outputId": "98eed11e-c0da-444b-e97a-80f9e02ec175"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "829112\n",
            "59464\n",
            "13380994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_window = len(windows)\n",
        "\n",
        "for key in word_pair_count:\n",
        "    temp = key.split(',')\n",
        "    i = int(temp[0])\n",
        "    j = int(temp[1])\n",
        "    # print(\"pair(i,j)=\",i,j)\n",
        "    count = word_pair_count[key] #p(i,j)\n",
        "    word_freq_i = word_window_freq[vocab_words_list[i]] #p(i)\n",
        "    word_freq_j = word_window_freq[vocab_words_list[j]] #p(j)\n",
        "    pmi = math.log((1.0 * count / num_window) /(1.0 * word_freq_i * word_freq_j/(num_window * num_window)),10) #adj(i,j)\n",
        "    if pmi <= 0:\n",
        "        continue\n",
        "    # Adj_Matrix[docs_size+i][docs_size+j]=pmi\n",
        "    weights.append(pmi)\n",
        "    row_list.append(docs_size+i)\n",
        "    col_list.append(docs_size+j)\n"
      ],
      "metadata": {
        "id": "7x8TSh9qd3cw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(weights[:5])\n",
        "print(row_list[:5])\n",
        "print(col_list[:5])"
      ],
      "metadata": {
        "id": "1TvYbxBtzHBy",
        "outputId": "eceb3474-143a-4da6-fc76-bc70fd365581",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.28479792351521793, 0.28479792351521793, 0.2759803469745833, 0.2759803469745833, 0.3413138889916103]\n",
            "[19693, 40771, 19693, 44805, 17238]\n",
            "[40771, 19693, 44805, 19693, 40771]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/MyDrive/IR_Project/dataset/pmi.csv', 'w', newline='') \n",
        "writer = csv.writer(file) \n",
        "writer.writerow([\"weight\",\"row\",\"col\"])\n",
        "for i,weight_pmi in enumerate(weights):\n",
        "  line=[]\n",
        "  line.append(weights[i])\n",
        "  line.append(row_list[i])\n",
        "  line.append(col_list[i])\n",
        "  writer.writerow(line)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "bXRekUt-mD6-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/MyDrive/IR_Project/dataset/pmi.csv',encoding='Latin1')\n",
        "type(file)\n",
        "csvreader = csv.reader(file)\n",
        "\n",
        "header = []\n",
        "header = next(csvreader)\n"
      ],
      "metadata": {
        "id": "9PZzkXhUxpSp"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights=[]\n",
        "row_list=[]\n",
        "col_list=[]\n",
        "count_skip=0\n",
        "for r in csvreader:\n",
        "    weights.append(float(r[0]))\n",
        "    row_list.append(int(r[1]))\n",
        "    col_list.append(int(r[2]))\n",
        "file.close()\n",
        "print(weights[:5])\n",
        "print(row_list[:5])\n",
        "print(col_list[:5])\n"
      ],
      "metadata": {
        "id": "mesvfemcx86-",
        "outputId": "b9bfd346-1804-4011-be29-bafd5e3a9997",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.28479792351521793, 0.28479792351521793, 0.2759803469745833, 0.2759803469745833, 0.3413138889916103]\n",
            "[19693, 40771, 19693, 44805, 17238]\n",
            "[40771, 19693, 44805, 19693, 40771]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "Adj_Mat=csr_matrix(Adj_Matrix)"
      ],
      "metadata": {
        "id": "KkgCixAQrhs2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Adj_Matrix = sp.csr_matrix((weights, (row_list, col_list)), shape=(docs_size+len(vocab_words_list), docs_size+len(vocab_words_list)))\n",
        "\n",
        "# build symmetric adjacency matrix\n",
        "Adj_Matrix = Adj_Matrix + Adj_Matrix.T.multiply(Adj_Matrix.T > Adj_Matrix) - Adj_Matrix.multiply(Adj_Matrix.T > Adj_Matrix)"
      ],
      "metadata": {
        "id": "pBawb0WrQYwT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Adj_Matrix=Adj_Mat+Adj_Matrix"
      ],
      "metadata": {
        "id": "8iF4mNfOv62N"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Adj_Matrix=csr_matrix(Adj_Matrix)"
      ],
      "metadata": {
        "id": "G__jRVAVwB-0"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr=np.array([[1,2,3],[1,2,3],[2,2,2]])\n",
        "print(arr)\n",
        "row = np.array([0, 1, 2, 0])\n",
        "col = np.array([0, 1, 1, 0])\n",
        "data = np.array([1, 2, 4, 8])\n",
        "arr=csr_matrix((data, (row, col)), shape=(3, 3)).toarray()\n",
        "print(arr)"
      ],
      "metadata": {
        "id": "4R5Fv_uDtXNm",
        "outputId": "a65bbc83-d259-4d76-a554-1b1ea973525e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [1 2 3]\n",
            " [2 2 2]]\n",
            "[[9 0 0]\n",
            " [0 2 0]\n",
            " [0 4 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr2=np.array([[1,2,3],[1,2,3],[2,2,2]])\n",
        "print(arr2)\n",
        "row = np.array([0, 1, 2, 0])\n",
        "col = np.array([0, 1, 1, 0])\n",
        "data = np.array([1, 2, 4, 8])\n",
        "arr2=csr_matrix((data, (row, col)), shape=(3, 3)).toarray()\n",
        "print(arr2)"
      ],
      "metadata": {
        "id": "xxHoKpzdtsn7",
        "outputId": "992fab59-1431-4a5d-e903-20f060bc6925",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [1 2 3]\n",
            " [2 2 2]]\n",
            "[[9 0 0]\n",
            " [0 2 0]\n",
            " [0 4 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import scipy.sparse.*_matrix\n",
        "arr2=arr2+arr\n",
        "print(arr2)"
      ],
      "metadata": {
        "id": "CAJXeqc_vB2A",
        "outputId": "267f675e-3790-4484-da6b-21a6217e8b63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[27  0  0]\n",
            " [ 0  6  0]\n",
            " [ 0 12  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Yu5SwTpx5ysl"
      }
    }
  ]
}